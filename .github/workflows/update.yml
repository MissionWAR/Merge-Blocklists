name: Merge Blocklists

on:
  workflow_dispatch:
  schedule:
    - cron: "0 */12 * * *"   # Every 12 hours (00:00, 12:00 UTC)

env:
  RAW_OUT: lists/_raw
  MERGED_OUT: lists/merged.txt
  FETCH_CONCURRENCY: 8
  FETCH_RETRIES: 3
  FETCH_TIMEOUT: 30
  PYTHON_VERSION: "3.11"

permissions:
  contents: write

# Ensure only a single workflow run is active at a time
concurrency:
  group: merge-blocklists
  cancel-in-progress: true

jobs:
  update:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      # ============================================
      # Setup
      # ============================================
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 1  # Shallow clone for faster checkout

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --no-cache-dir -r requirements.txt
          echo "âœ… Dependencies installed"

      # ============================================
      # Cache slot (weekly)
      # ============================================
      - name: Compute cache slot (weekly)
        id: cache-slot
        run: |
          # ISO year-week, for example: 2025-07
          echo "CACHE_SLOT=$(date -u +%G-%V)" >> $GITHUB_ENV

      # ============================================
      # Caching
      # ============================================
      - name: Restore blocklist caches
        uses: actions/cache@v4
        with:
          path: |
            .cache
          # Weekly cache prefix so runs within the same week share a single cache artifact
          key: blocklists-cache-v1-${{ env.CACHE_SLOT }}
          restore-keys: |
            blocklists-cache-v1-${{ env.CACHE_SLOT }}-

      # ============================================
      # Fetch & Process
      # ============================================
      - name: Fetch blocklist sources
        id: fetch
        run: |
          set -euo pipefail
          echo "ðŸ”„ Fetching blocklists from sources..."

          fetch_start=$(date +%s)
          
          python -m scripts.fetch_sources \
            --sources sources.txt \
            --outdir "${{ env.RAW_OUT }}" \
            --cache .cache \
            --concurrency "${{ env.FETCH_CONCURRENCY }}" \
            --retries "${{ env.FETCH_RETRIES }}" \
            --timeout "${{ env.FETCH_TIMEOUT }}"

          fetch_end=$(date +%s)
          echo "fetch_seconds=$((fetch_end - fetch_start))" >> $GITHUB_OUTPUT
          
          # Count fetched files
          RAW_COUNT=$(find "${{ env.RAW_OUT }}" -type f -name "*.txt" | wc -l)
          echo "raw_files=$RAW_COUNT" >> $GITHUB_OUTPUT
          echo "âœ… Fetched $RAW_COUNT source files"

      - name: Process blocklists through pipeline
        id: pipeline
        run: |
          set -euo pipefail
          echo "âš™ï¸ Processing through pipeline..."

          pipeline_start=$(date +%s)
          
          # Run the full pipeline (uses .pipeline_cache internally when available)
          python -m scripts.pipeline "${{ env.RAW_OUT }}" "${{ env.MERGED_OUT }}"

          pipeline_end=$(date +%s)
          echo "pipeline_seconds=$((pipeline_end - pipeline_start))" >> $GITHUB_OUTPUT
          
          # Verify output exists
          if [ ! -f "${{ env.MERGED_OUT }}" ]; then
            echo "âŒ ERROR: Merged output file not created"
            exit 1
          fi
          
          echo "âœ… Pipeline completed successfully"

      - name: Calculate statistics
        id: stats
        run: |
          set -euo pipefail
          
          # Count total rules (non-comment, non-empty lines)
          TOTAL_RULES=$(grep -cvE '^\s*($|[#!])' "${{ env.MERGED_OUT }}" || echo "0")
          
          # Get file size
          FILE_SIZE=$(du -h "${{ env.MERGED_OUT }}" | cut -f1)
          
          # Count cache entries
          CACHE_FILES=$(find .cache -type f -name "*.txt" 2>/dev/null | wc -l || echo "0")
          
          # Output to step outputs and environment
          echo "total_rules=$TOTAL_RULES" >> $GITHUB_OUTPUT
          echo "file_size=$FILE_SIZE" >> $GITHUB_OUTPUT
          echo "TOTAL_RULES=$TOTAL_RULES" >> $GITHUB_ENV
          
          echo "ðŸ“Š Statistics:"
          echo "  - Total rules: $TOTAL_RULES"
          echo "  - File size: $FILE_SIZE"
          echo "  - Cached sources: $CACHE_FILES"

      # ============================================
      # Validation
      # ============================================
      - name: Validate output
        run: |
          set -euo pipefail
          
          # Check minimum rule count (prevent empty or broken releases)
          MIN_RULES=1000
          if [ "${{ steps.stats.outputs.total_rules }}" -lt "$MIN_RULES" ]; then
            echo "âš ï¸ WARNING: Only ${{ steps.stats.outputs.total_rules }} rules (expected >$MIN_RULES)"
            echo "This might indicate a fetch/processing issue"
            exit 1
          fi
          
          echo "âœ… Output validated (${{ steps.stats.outputs.total_rules }} rules)"

      # ============================================
      # Release
      # ============================================
      - name: Generate release notes
        id: release_notes
        if: steps.stats.outputs.total_rules != '0'
        run: |
          cat > release_notes.md << EOF
          ## Merged Blocklist
          
          **Updated**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          
          ### Statistics
          - **Total Rules**: ${{ steps.stats.outputs.total_rules }}
          - **File Size**: ${{ steps.stats.outputs.file_size }}
          - **Sources Processed**: ${{ steps.fetch.outputs.raw_files }}
          EOF
          
          echo "âœ… Release notes generated"

      - name: Create or update release
        if: steps.stats.outputs.total_rules != '0'
        uses: softprops/action-gh-release@v2
        with:
          tag_name: merged-latest
          name: Merged Blocklist (Latest)
          body_path: release_notes.md
          files: ${{ env.MERGED_OUT }}
          draft: false
          prerelease: false
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # ============================================
      # Cleanup & Summary
      # ============================================
      - name: Cleanup old cache entries
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up old cache entries (>7 days)..."
          find .cache -type f -mtime +7 -delete 2>/dev/null || true
          
          # Clean up temporary raw files (keep caches)
          rm -rf "${{ env.RAW_OUT }}" 2>/dev/null || true
          
          echo "âœ… Cleanup completed"

      - name: Generate job summary
        if: always()
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # ðŸ“‹ Blocklist Update Summary
          
          ## Status
          - **Workflow**: ${{ job.status }}
          
          ## Statistics
          | Metric | Value |
          |--------|-------|
          | Total Rules | ${{ steps.stats.outputs.total_rules || 'N/A' }} |
          | File Size | ${{ steps.stats.outputs.file_size || 'N/A' }} |
          | Sources Fetched | ${{ steps.fetch.outputs.raw_files || 'N/A' }} |
          | Fetch Duration (s) | ${{ steps.fetch.outputs.fetch_seconds || 'N/A' }} |
          | Pipeline Duration (s) | ${{ steps.pipeline.outputs.pipeline_seconds || 'N/A' }} |
          
          ## Timestamps
          - **Started**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          - **Run ID**: ${{ github.run_id }}
          - **Commit**: \`${{ github.sha }}\`
          
          ## Next Update
          Scheduled for: **12 hours from now**
          EOF
          
          echo "âœ… Job summary generated"
